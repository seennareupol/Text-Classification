{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJFNuBMv8s6z",
        "outputId": "274a8e20-43f9-4e27-9cd3-fe5a9ed3a554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (2.2.6)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: torch in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (4.57.3)\n",
            "Requirement already satisfied: x-transformers in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (2.14.2)\n",
            "Requirement already satisfied: einops in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: einx>=0.3.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from x-transformers) (0.3.0)\n",
            "Requirement already satisfied: loguru in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from x-transformers) (0.7.3)\n",
            "Requirement already satisfied: frozendict in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from einx>=0.3.0->x-transformers) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from loguru->x-transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive\\desktop\\seen_jupyter\\venv\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas numpy scikit-learn torch transformers x-transformers einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_m-FzCGGu8Zz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\OneDrive\\Desktop\\seen_jupyter\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer\n",
        "from x_transformers import TransformerWrapper, Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hy2T7hEYCPHT"
      },
      "outputs": [],
      "source": [
        "CSV_PATH = \"C:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\seen_jupyter\\\\research\\\\archive\\\\prachatai_train.csv\"\n",
        "MODEL_PATH = \"xtransformer_best.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KJzFvWdBCPzL"
      },
      "outputs": [],
      "source": [
        "# Batch size = how many samples are used to compute ONE weight update(optimizer.step())\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 64 # a memory workaround, not a performance boost.\n",
        "ACCUM_STEPS = 2\n",
        "EPOCHS = 500\n",
        "PATIENCE = 15\n",
        "LR = 2e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yNnAmflYCT4Y"
      },
      "outputs": [],
      "source": [
        "LABEL_COLS = [\n",
        "    \"politics\", \"human_rights\", \"quality_of_life\", \"international\",\n",
        "    \"social\", \"environment\", \"economics\", \"culture\", \"labor\",\n",
        "    \"national_security\", \"ict\", \"education\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEr1WN7BwjJ5",
        "outputId": "9843dcbb-b7fb-41d6-add0-1c2ce1ce631b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YziWwCizwjHp",
        "outputId": "84822784-8d49-4be3-af53-769d2b5f5197"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
        ")\n",
        "PAD_ID = tokenizer.pad_token_id # in this work PAD_ID = 1, The token ID reserved for [PAD], used to fill shorter sequences so all samples in a batch have the same length and should be ignored by the model via masking.\n",
        "VOCAB_SIZE = tokenizer.vocab_size # Total number of tokens the model knows; valid token IDs range from 0 to vocab_size − 1 and define the size of the embedding table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "muM0OrzJwjFJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(CSV_PATH)\n",
        "texts = df[\"body_text\"].astype(str).tolist()\n",
        "labels = df[LABEL_COLS].values.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nxkcvPMrwjCj"
      },
      "outputs": [],
      "source": [
        "# Encoding converts text into token IDs (start with special tokens like [CLS]), truncates it to a fixed max length, and feeds the resulting numbers into the model so it can process language.\n",
        "def encode_texts(texts):\n",
        "    enc = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=False\n",
        "    )\n",
        "    return enc[\"input_ids\"]\n",
        "\n",
        "encoded_texts = encode_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2pf3AGcJBfLE"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    encoded_texts, labels, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EdT9093M9Rk0"
      },
      "outputs": [],
      "source": [
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    encoded_texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.5, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vXO-PqZoDFI8"
      },
      "outputs": [],
      "source": [
        "class ThaiTextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]), self.y[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    seqs, labels = zip(*batch)\n",
        "    padded = pad_sequence(seqs, batch_first=True, padding_value=PAD_ID) # batch_first=True = เอา batch ไว้แกนแรก (มิติที่ 0) ทำให้แสดง shape ใช้ง่ายขึ้น\n",
        "    return padded, torch.stack(labels) # shape input data (batch_size, seq_len), shape labels (batch_size, num_labels)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ThaiTextDataset(X_train, y_train),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    ThaiTextDataset(X_val, y_val),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    ThaiTextDataset(X_test, y_test),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "93X-rwq-wifW"
      },
      "outputs": [],
      "source": [
        "\"\"\"Input x (token_ids)\n",
        "        ↓\n",
        "Embedding lookup\n",
        "        ↓\n",
        "Positional embedding\n",
        "        ↓\n",
        "Transformer Decoder (attention layers)\n",
        "        ↓\n",
        "Per-token embeddings h  (B, T, 256)\n",
        "        ↓\n",
        "Mask padding tokens (PAD → 0)\n",
        "        ↓\n",
        "Mean pooling over tokens\n",
        "        ↓\n",
        "Sentence embedding (B, 256)\n",
        "        ↓\n",
        "Linear classifier\n",
        "        ↓\n",
        "Logits (B, num_labels)\"\"\"\n",
        "class XTransformerClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = TransformerWrapper(\n",
        "            num_tokens=VOCAB_SIZE, # size of embedding table\n",
        "            max_seq_len=MAX_LEN, # defines positional embeddings + attention limits\n",
        "            attn_layers=Decoder(\n",
        "                dim=256, # vector dimension of the model\n",
        "                depth=4, # number of transformer blocks like layers\n",
        "                heads=4 # attention heads (thinking viewpoints) \n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256, len(LABEL_COLS))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.transformer(x, return_embeddings=True) # h = (batch_size, seq_len, dim) dim=256\n",
        "        mask = (x != PAD_ID).unsqueeze(-1) # mask = (batch_size, seq_len, 1)\n",
        "\n",
        "        h = h * mask # PAD vectors are zeroed out.\n",
        "        pooled = h.sum(1) / mask.sum(1).clamp(min=1) # mean pooling (Sentence Embedding)\n",
        "\n",
        "        return self.classifier(pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rgn2inhNvPc7"
      },
      "outputs": [],
      "source": [
        "model = XTransformerClassifier().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cqc7a0sy42uC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\3621505036.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.BCEWithLogitsLoss() # loss function for multi-label classification\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR) # update model weights based on computed gradients\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GeGrPzI442rp"
      },
      "outputs": [],
      "source": [
        "# Remember best scores, Stop training when validation performance stops improving\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience):\n",
        "        self.best = 0\n",
        "        self.counter = 0\n",
        "        self.patience = patience\n",
        "\n",
        "    def step(self, score, model):\n",
        "        if score > self.best:\n",
        "            self.best = score\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            return self.counter >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"for epoch\n",
        "  ├─ model.train()\n",
        "  ├─ for batch in train_loader\n",
        "  │     ├─ Forward pass (autocast)\n",
        "  │     ├─ Compute loss / ACCUM_STEPS\n",
        "  │     ├─ Backward (scaled, accumulate gradients)\n",
        "  │     ├─ Every ACCUM_STEPS:\n",
        "  │     │     ├─ optimizer.step()\n",
        "  │     │     ├─ scaler.update()\n",
        "  │     │     └─ zero gradients\n",
        "  └─ End training loop\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c4uOzN8P42pY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Loss 178.1761 | Val F1 0.4923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 002 | Loss 128.3318 | Val F1 0.5555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 003 | Loss 108.7233 | Val F1 0.6013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 004 | Loss 93.1894 | Val F1 0.6217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 005 | Loss 78.5428 | Val F1 0.6098\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 006 | Loss 64.8302 | Val F1 0.6091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 007 | Loss 52.2792 | Val F1 0.6087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 008 | Loss 40.8095 | Val F1 0.5931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 009 | Loss 31.2464 | Val F1 0.5948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 010 | Loss 23.0007 | Val F1 0.5875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 011 | Loss 16.8815 | Val F1 0.5911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 012 | Loss 12.3902 | Val F1 0.5809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 013 | Loss 9.3248 | Val F1 0.5823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 014 | Loss 7.9260 | Val F1 0.5851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 015 | Loss 6.8222 | Val F1 0.5846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 016 | Loss 6.4968 | Val F1 0.5852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 017 | Loss 6.0898 | Val F1 0.5867\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 018 | Loss 5.3336 | Val F1 0.5887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29756\\1984414586.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 019 | Loss 5.4850 | Val F1 0.5838\n",
            "⏹ Early stopping\n"
          ]
        }
      ],
      "source": [
        "early_stop = EarlyStopping(PATIENCE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # -------- TRAIN --------\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, (Xb, yb) in enumerate(train_loader):\n",
        "        Xb = Xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(): # Runs forward pass in float16 where safe, if not use float32\n",
        "            preds = model(Xb)\n",
        "            loss = criterion(preds, yb) / ACCUM_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward() # avoid float16 issues by multiplying gradients by big number\n",
        "\n",
        "        if (step + 1) % ACCUM_STEPS == 0:\n",
        "            scaler.step(optimizer) # unscales gradient (divided big number), checks for inf/NaN, calls optimizer.step()\n",
        "            scaler.update() # adjusts the scale for next iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * ACCUM_STEPS\n",
        "\n",
        "    # -------- VALIDATION --------\n",
        "    model.eval() \n",
        "    yt, yp = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)  \n",
        "            probs = torch.sigmoid(model(Xb))\n",
        "            preds = (probs > 0.5).int()\n",
        "            yt.append(yb.cpu().numpy())\n",
        "            yp.append(preds.cpu().numpy())\n",
        "\n",
        "    yt = np.vstack(yt)\n",
        "    yp = np.vstack(yp)\n",
        " \n",
        "    val_f1 = f1_score(yt, yp, average=\"macro\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:03d} | \"\n",
        "        f\"Loss {total_loss:.4f} | \"\n",
        "        f\"Val F1 {val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    if early_stop.step(val_f1, model):\n",
        "        print(\"⏹ Early stopping\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxsSNiaY42mh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FINAL TEST RESULTS\n",
            "Macro F1: 0.6143399543094565\n",
            "Micro F1: 0.6931865765388526\n",
            "politics 0.8246784958489337\n",
            "human_rights 0.6447656592203241\n",
            "quality_of_life 0.6430180180180181\n",
            "international 0.7411487018095987\n",
            "social 0.31\n",
            "environment 0.7668202764976959\n",
            "economics 0.5466491458607096\n",
            "culture 0.5520833333333334\n",
            "labor 0.7622641509433963\n",
            "national_security 0.4642857142857143\n",
            "ict 0.6271186440677966\n",
            "education 0.489247311827957\n"
          ]
        }
      ],
      "source": [
        " \n",
        "model.eval()\n",
        "\n",
        "yt, yp = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device, non_blocking=True)\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "        probs = torch.sigmoid(model(Xb))\n",
        "        preds = (probs > 0.5).int()\n",
        "        yt.append(yb.cpu().numpy())\n",
        "        yp.append(preds.cpu().numpy())\n",
        "\n",
        "yt = np.vstack(yt)\n",
        "yp = np.vstack(yp)\n",
        "\n",
        "print(\"\\nFINAL TEST RESULTS\")\n",
        "print(\"Macro F1:\", f1_score(yt, yp, average=\"macro\"))\n",
        "print(\"Micro F1:\", f1_score(yt, yp, average=\"micro\"))\n",
        "\n",
        "for i, label in enumerate(LABEL_COLS):\n",
        "    print(label, f1_score(yt[:, i], yp[:, i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF8X_kcU42jm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PREDICT EXAMPLES\n",
            "LOGITS: [-1.1143008   0.03629711 -3.460578    4.9665728  -4.713972   -5.813804\n",
            " -4.7403603  -6.28018    -4.9642787  -4.3968506  -3.2291167  -4.738405  ]\n",
            "PROBS : [0.24706995 0.50907326 0.03045497 0.9930813  0.00888935 0.00297716\n",
            " 0.00865985 0.00186956 0.00693456 0.01216623 0.03808459 0.00867665]\n",
            "[('international', 0.993081271648407)]\n",
            "LOGITS: [-0.9710651 -1.9250987 -3.7720175 -3.1786907 -3.6883214 -5.855277\n",
            " -5.8628187 -6.639396  -3.15854   -6.7254477 -9.345167  -7.149972 ]\n",
            "PROBS : [2.7466825e-01 1.2729408e-01 2.2488248e-02 3.9975554e-02 2.4403527e-02\n",
            " 2.8565584e-03 2.8351571e-03 1.3061085e-03 4.0756091e-02 1.1985450e-03\n",
            " 8.7379100e-05 7.8427052e-04]\n",
            "[('politics', 0.27466824650764465)]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "def predict(text, threshold=0.5, k=1):\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    x = enc[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0]\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        print(\"LOGITS:\", logits.cpu().numpy())\n",
        "        print(\"PROBS :\", probs)\n",
        "\n",
        "    return sorted(\n",
        "        # [(LABEL_COLS[i], float(probs[i])) for i in range(len(LABEL_COLS)) if probs[i] >= threshold],\n",
        "        [(LABEL_COLS[i], float(probs[i])) for i in range(len(LABEL_COLS))],\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )[:k]\n",
        "\n",
        "print(\"\\nPREDICT EXAMPLES\")\n",
        "print(predict(\"\"\"https://prachatai.com/print/74297,2017-11-26 09:17,รัตโก มลาดิช ทหารใหญ่กองกำลังชาวเซิร์บถูกตัดสินมีความผิดฐานฆ่าล้างเผ่าพันธุ์,\"อดีตเสนาธิการกองทัพเซิร์บบอสเนีย รัตโก มลาดิช ถูกตัดสินให้มีความผิดฐานฆ่าล้างเผ่าพันธุ์ อาชญากรรมต่อมนุษยชาติ และโทษฐานละเมิดสิทธิมนุษยชนอื่นๆ จากเหตุการณ์สังหารหมู่ กวาดต้อนจับกุมผู้คนและไล่ผู้คนออกจากพื้นที่ในช่วงปี 2535-2538 รวมถึงมีเหตุการณ์จับเจ้าหน้าที่สหประชาชาติเป็นตัวประกันด้วย\n",
        " \n",
        "25 พ.ย. 2560 ในการพิจารณาขั้นสุดท้าย ศาลอาญาระหว่างประเทศเพื่อพิจารณาคดีอดีตผู้นำยูโกสลาเวีย (ICTY) ตัดสินให้ รัตโก มลาดิช อดีตเสนาธิการกองทัพเซิร์บบอสเนีย มีความผิดฐานฆ่าล้างเผ่าพันธุ์ อาชญากรรมต่อมนุษยชาติและละเมิดกฎหมายสงครามหรือธรรมเนียมสงคราม จากอาชญากรรมที่เขาและกองกำลังเชิร์บก่อไว้ในช่วงยุคสงครามบอสเนียระหว่างปี 2535-2538 ทำให้เขาถูกลงโทษด้วยการจำคุกตลอดชีวิต\n",
        " \n",
        "มลาดิช ถูกตัดสินว่ามีความผิดจริงในโทษฐานฆ่าล้างเผ่าพันธุ์ รวมถึงการใช้กำลังปราบปราม กำจัด สังหาร และปฏิบัติอย่างไร้มนุษยธรรมในการบังคับย้ายถิ่นฐานประชาชนในพื้นที่สเรเบรนีตซาในปี 2538 และการกระทำโหดร้ายอื่นๆ เช่นการสังหาร ก่อการร้ายต่อพลเรือนในซาราเยโว รวมถึงมีการจับเจ้าหน้าที่สหประชาชาติ (ยูเอ็น) เป็นตัวประกัน\n",
        " \n",
        "ก่อนหน้านี้ มลาดิชเคยถูกตัดสินให้พ้นจากความผิดในกรณีฆ่าล้างเผ่าพันธุ์ในหลายพื้นที่ของบอสเนียและเฮอร์เซโกวีนาช่วงปี 2535 อย่างไรก็ตาม ในการตัดสินครั้งล่าสุดศาล ICTY ระบุว่ามลาดิชเคยเข้าร่วมหรือให้การสนับสนุนกลุ่มร่วมมือก่ออาชญากรรมใน 4 พื้นที่ กลุ่มเหล่านี้มีอยู่ในช่วงยุคก่อนสงครามบอสเนียจนถึงสิ้นสุดสงคราม เป็นกลุ่มที่พยายามกวาดล้างเผ่าพันธุ์ชาวมุสลิม ชาวโครแอต ออกจากพื้นที่ที่ชาวเซิร์บอ้างว่าเป็นของตน\n",
        " \n",
        "ผู้พิพากษากล่าวว่ากองกำลังชาวเซิร์บสังหารชาวมุสลิมและชาวโครแอตของบอสเนียจำนวนมาก นอกจากนี้ยังมีบางส่วนที่บังคับย้ายถิ่นฐานจากบ้านตัวเอง ผู้พิพากษา อัลฟอนส์ ออร์รี กล่าวว่าเป็นสภาพการณ์ที่โหดร้าย คนที่พยายามปกป้องบ้านตัวเองถูกใช้ความรุนแรงอย่างไร้ปราณี มีการสังหารหมู่ บางคนถูกทุบตี ผู้ก่อเหตุหลายคนที่จับตัวชาวมุสลิมไว้แสดงออกให้เห็นว่าไม่ได้เคารพในศักดิ์ศรีหรือชีวิตของมนุษย์เลย\n",
        " \n",
        "อาชญากรรมที่ทหารระดับสูงผู้นี้เคยก่อไว้ยังรวมไปถึงการจับกุมคุมขังผู้คนไว้ในเรือนจำ ซึ่งมักจะอยู่ภายใต้สภาพแวดล้อมที่ป่าเถื่อน ผู้ต้องขังมักจะถูกทารุณกรรม ทุบตีทำร้าย ข่มขืน รวมถึงถูกกระทำความรุนแรงทางเพศ หลังจากนั้นจึงถูกส่งตัวออกนอกเขตปกครอง\n",
        " \n",
        "โดยที่มลาดิชเป็นตัวการใหญ่ของการก่อเหตุเหล่านี้ทั้งหมด ในแง่ของการสร้างเป้าหมายให้มีการขจัดคนเชื้อชาติอื่นออกจากพื้นที่บอสเนียและเฮอร์เซโกวีนา รวมถึงเกื้อหนุนกลุ่มอาชญากรในการก่อการร้ายอย่างการใช้สไนเปอร์และยิงอาวุธระเบิดเพื่อสร้างความหวาดผวาแก่ประชาชนในซาราเยโว ทำให้มีประชาชนเสียชีวิตและได้รับบาดเจ็บเรือนหมื่น ประชาชนเต็มไปด้วยความเดือดร้อนกลัวว่าคนที่พวกเขารักจะถูกยิงด้วยสไนเปอร์หรือถูกระเบิด นั่นทำให้มลาดิชถูกตัดสินให้มีความผิดฐานก่อการร้าย โจมตีพลเรือนอย่างผิดกฎหมาย และฆาตกรรม\n",
        " \n",
        "นอกจากนี้มลาดิชยังเคยก่อตั้งกลุ่มอาชญากรที่เอาไว้จับเจ้าหน้าที่ยูเอ็นเป็นตัวประกันโดยเฉพาะเพื่อบีบเค้นให้นาโตหยุดการโจมตีทางอากาศต่อพื้นที่ของกลุ่มบอสเนียเซิร์บ ซึ่งเขาถูกตัดสินให้มีความผิดฐานจับคนเป็นตัวประกันด้วย\n",
        " \n",
        "ในกรณีสังหารหมู่ปี 2538 ศาลระบุว่ามลาดิชเข้าร่วมในการกวาดล้างกลุ่มชาวบอสเนียมุสลิม มีการบังคับไล่ที่เด็ก ผู้หญิง และคนชรา ชาวบอสเนียมุสลิมออกจากพื้นที่และมีการจับตัวชายชาวบอสเนียมุสลิมจากฐานทัพของยูเอ็นไปคุมขังชั่วคราว ก่อนจะนำขึ้นรถโดยสารไปพร้อมกับกลุ่มคนที่พยายามหลบหนีเพื่อไปสังหารตามที่ต่างๆ ในกรณีนี้ทำให้มลาดิชถูกตัดสินให้มีความผิดฐานฆ่าล้างเผ่าพันธุ์ ล่าสังหาร ฆาตกรรม ทำลายล้าง และบังคับให้ออกจากพื้นที่โดยไร้มนุษยธรรม\n",
        " \n",
        "กลุ่มที่ถูกตัดสินคดีในครั้งนี้มีสิทธิยื่นอุทธรณ์คำตัดสิน โดยถ้าหากมีการรับอุทธรณ์จะมีการพิจารณาคดีอีกครั้งในชั้นศาลอาญาระหว่างประเทศเพื่อกลไกตกค้าง (International Residual Mechanism for Criminal Tribunals หรือ MICT) \n",
        " \n",
        "ถึงแม้เหตุการณ์จะเกิดขึ้นตั้งแต่ราว 25 ปีที่แล้วแต่ก็เพิ่งมีการดำเนินคดีเมื่อปี 2555 มีการไต่สวนพยาน 592 ปาก และสืบหลักฐานเกือบ 10,000 ชั้น มีการไต่สวนคดีรวมเวลา 530 วัน ในช่วง 4-5 ปีที่ผ่านมา มีการวินิจฉัยข้อเท็จจริงราว 2,000 ชิ้น และมีการพิสูจน์โต้แย้งในขั้นตอนสุดท้ายเมื่อปลายปี 2559\n",
        " \n",
        "นังตั้งแต่มีการก่อตั้ง ICTY ศาลนี้สั่งตัดสินผู้คนที่ละเมิดกฎมนุษยธรรมไปแล้ว 161 ราย จากความขัดแย้งในพื้นที่ยูโกสลาเวียช่วงปี 2534 ถึง 2544 มีการสรุปคดีไปแล้ว 155 คดี และมีที่อยู่ระหว่างกระบวนการ 6 คดี\n",
        " \n",
        " \n",
        "เรียบเรียงจาก\n",
        " \n",
        "ICTY convicts Ratko Mladić for genocide, war crimes and crimes against humanity, UN ICTY, 22-11-2017\n",
        "http://www.icty.org/en/press/tribunal-convicts-ratko-mladi%C4%87-for-genocide-war-crimes-and-crimes-against-humanity [1]\"\"\", k=1))\n",
        "print(predict(\"แรงงานเรียกร้องสิทธิ์การทำงาน\", k=1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (seen_jupyter venv1)",
      "language": "python",
      "name": "seen-jupyter-venv1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
