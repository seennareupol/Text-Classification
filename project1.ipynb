{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e43884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy_thai\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1b7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pythainlp in c:\\python310\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: tzdata in c:\\python310\\lib\\site-packages (from pythainlp) (2025.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\python310\\lib\\site-packages (from pythainlp) (2.32.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.31->pythainlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.31->pythainlp) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python310\\lib\\site-packages (from requests>=2.31->pythainlp) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.31->pythainlp) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pythainlp\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc07b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\OneDrive\\\\Desktop\\\\seen_jupyter\\\\venv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79ceb563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6789, 17)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\seen_jupyter\\research\\prachatai(TK Edition).csv')\n",
    "df_train = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\seen_jupyter\\research\\archive\\prachatai_train.csv')\n",
    "df_valid = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\seen_jupyter\\research\\archive\\prachatai_validation.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\seen_jupyter\\research\\archive\\prachatai_test.csv')\n",
    "df_train.shape\n",
    "df_valid.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f008bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 54379\n",
      "Y shape: (54379, 12)\n",
      "X shape: 6721\n",
      "Y shape: (6721, 12)\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.corpus.common import thai_stopwords\n",
    "\n",
    "stop = set(thai_stopwords())\n",
    "n_head = 3000\n",
    "label_cols = [\"politics\",\"human_rights\",\"quality_of_life\",\"international\",\"social\",\"environment\",\"economics\",\"culture\",\"labor\",\"national_security\",\"ict\",\"education\"]\n",
    "  \n",
    "def split_IO(dataframe):\n",
    "    # X =  df[\"body_text\"].fillna(\"\").astype(str).head(n_head).tolist() #[\"...\", \"...\", \"...\"] ลด NaN\n",
    "    X =  dataframe[\"body_text\"].fillna(\"\").astype(str).tolist() #[\"...\", \"...\", \"...\"] ลด NaN\n",
    "\n",
    "    Y = dataframe[label_cols].fillna(0).astype(np.float32).to_numpy()\n",
    "    print(\"X shape:\", len(X))\n",
    "    print(\"Y shape:\", Y.shape)\n",
    "    assert len(X) == Y.shape[0]\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = split_IO(df_train)\n",
    "X_valid, Y_valid = split_IO(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fba3f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pythainlp.util import normalize\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "\n",
    "stop = set(thai_stopwords())\n",
    "\n",
    "# --- กรอง token ให้อยู่ในรูปที่ model ใช้ง่าย ---\n",
    "_token_ok = re.compile(r\"^[ก-๙a-zA-Z0-9]+$\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.replace(\"\\xa0\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    s = normalize(s)\n",
    "    s = s.replace('\"', \" \").replace('”', \" \").replace(\"''\", \" \")   \n",
    "    # ยุบช่องว่างหลายๆอันให้เหลือช่องว่างเดียว\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "     \n",
    "\n",
    "def preprocessing(list_text, engine=\"newmm\", min_len=2, keep_num=True):\n",
    "    tovec_list = []\n",
    "    tfidf_list = []\n",
    "    for text in list_text:\n",
    "        text = clean_text(text)\n",
    "        tokenized_data = word_tokenize(text, engine=engine)\n",
    "        cleaned_text = []\n",
    "        for tokenized_word in tokenized_data:\n",
    "            tokenized_word = tokenized_word.strip()\n",
    "\n",
    "            if not tokenized_word:\n",
    "                continue\n",
    "            if len(tokenized_word) < min_len:\n",
    "                continue\n",
    "            if tokenized_word in stop:\n",
    "                continue\n",
    "            if not _token_ok.match(tokenized_word):\n",
    "                continue\n",
    "            if not keep_num and tokenized_word.isdigit():\n",
    "                continue\n",
    "            cleaned_text.append(tokenized_word)\n",
    "\n",
    "        tovec_list.append(cleaned_text)\n",
    "        tfidf_list.append(\" \".join(cleaned_text))\n",
    "    return tovec_list, tfidf_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ded35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     ngram_range=(1,2),   # unigram, bigram \n",
    "#     min_df=2 # at least appear in 2 documents\n",
    "# )\n",
    "\n",
    "# tovec_list, tfidf_list = preprocessing(list_txt)\n",
    "# # print(tfidf_list)\n",
    "# outcome = vectorizer.fit_transform(tfidf_list)\n",
    "# print(outcome.shape)\n",
    "# sample = outcome[:100]\n",
    "# outcome_df = pd.DataFrame(sample.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# outcome_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7089282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thai2Vec\n",
    "from pythainlp.word_vector import WordVector\n",
    "\n",
    "class Thai2VecEncoder:\n",
    "    def __init__(self):\n",
    "        # โหลด pretrained model แค่ครั้งเดียว\n",
    "        self.wv = WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "        self.dim = self.wv.vector_size\n",
    "\n",
    "    def doc_vector(self, tokens: list):\n",
    "        \"\"\"\n",
    "        tokens: list[str] ของข่าว 1 ชิ้น\n",
    "        return: vector (dim,)\n",
    "        \"\"\"\n",
    "        vecs = []\n",
    "\n",
    "        for tok in tokens:\n",
    "            if tok in self.wv:\n",
    "                vecs.append(self.wv[tok])\n",
    "\n",
    "        if not vecs:\n",
    "            return np.zeros(self.dim, dtype=np.float32)\n",
    "\n",
    "        return np.mean(vecs, axis=0).astype(np.float32) # เฉลี่ยตามเเกนในเเนวตั้ง \n",
    "\n",
    "    def transform(self, tovec_list: list):\n",
    "        \"\"\"\n",
    "        tovec_list: list[list[str]]\n",
    "        return: X shape (n_docs, dim)\n",
    "        \"\"\"\n",
    "        return np.vstack([self.doc_vector(tokens) for tokens in tovec_list])\n",
    "    \n",
    "    def oov_rate(self, tovec_list, wv_model):\n",
    "        total = 0\n",
    "        oov = 0\n",
    "        for toks in tovec_list:\n",
    "            for t in toks:\n",
    "                total += 1\n",
    "                if t not in wv_model:\n",
    "                    oov += 1\n",
    "        return (oov / total) if total else 0.0\n",
    "\n",
    "\n",
    "# tovec_list, _ = preprocessing(X)\n",
    "\n",
    "# encoder = Thai2VecEncoder()\n",
    "\n",
    "# X = encoder.transform(tovec_list)\n",
    "\n",
    "# print(X)\n",
    "# print(np.isnan(X).any(), np.isinf(X).any())\n",
    "# print(\"row norms:\", np.linalg.norm(X, axis=1))\n",
    "# print(\"OOV rate:\", encoder.oov_rate(tovec_list, encoder.wv))\n",
    "# (จำนวนข่าว, 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3222c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e738ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) Dataset wrapper\n",
    "# -------------------------\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray):\n",
    "        \"\"\"\n",
    "        X: (n_docs, input_dim)  float\n",
    "        Y: (n_docs, n_labels)   0/1 float\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) MLP Model\n",
    "# -------------------------\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_labels: int, hidden_dim: int = 256, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Linear(hidden_dim // 2, n_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) Train / Eval loops\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad() # off backpropagation\n",
    "def eval_one_epoch(model, loader, criterion, device, threshold=0.5):\n",
    "    model.eval() # ปิด Dropout (ไม่สุ่ม) and make stable outcome\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "\n",
    "        logits = model(Xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "        preds = torch.sigmoid(logits)\n",
    "        pred = (preds >= threshold).int()  # 0/1 per label\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "    all_pred = np.concatenate(all_preds, axis=0)\n",
    "    all_true = np.concatenate(all_true, axis=0)\n",
    "    f1_micro = f1_score(all_true, all_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(all_true, all_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return total_loss / len(loader.dataset), f1_micro, f1_macro, all_true, all_pred\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) Main training function (connect to your code)\n",
    "# -------------------------\n",
    "def train_mlp_from_thai2vec(X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, label_names=None, batch_size=64, epochs=20, lr=1e-3, seed=42, threshold=0.5):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # typing check\n",
    "    assert X_train.ndim == 2 # must be 2D (n_docs, input_dim)\n",
    "    assert y_train.ndim == 2  # must be 2D (n_docs, n_labels)\n",
    "    assert X_train.shape[0] == y_train.shape[0] # row equal?\n",
    "    assert X_train.shape[1] == X_val.shape[1]\n",
    "    assert y_train.shape[1] == y_val.shape[1]\n",
    "    \n",
    "    n_labels = y_train.shape[1]\n",
    "    if label_names is None:\n",
    "        label_names = [f\"label_{i}\" for i in range(n_labels)]\n",
    "    else:\n",
    "        assert len(label_names) == n_labels # ensure label names match number of labels\n",
    "\n",
    "    train_ds = NumpyDataset(X_train, y_train)\n",
    "    val_ds = NumpyDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    input_dim = X_train.shape[1]    # should be 300\n",
    "    model = MLPClassifier(input_dim=input_dim, n_labels=n_labels).to(device)\n",
    "    pos = y_train.sum(axis=0)                 # จำนวน 1 ต่อ label\n",
    "    neg = y_train.shape[0] - pos              # จำนวน 0 ต่อ label\n",
    "    pos_weight = torch.tensor(neg / (pos + 1e-8), dtype=torch.float32).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # การพลาด label ที่หายาก ต้องโดนลงโทษหนักกว่าปกติ\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, f1_micro, f1_macro, y_true, y_pred = eval_one_epoch(model, val_loader, criterion, device, threshold) # evaluate with validationset 1 time \n",
    "\n",
    "        if f1_micro > best_val_acc:\n",
    "            best_val_acc = f1_micro\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()} # best_state[k] = v.cpu().clone()\n",
    "\n",
    "        print(f\"Training epochs: {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | f1_micro={f1_micro:.4f} | f1_macro={f1_macro:.4f}\")\n",
    "\n",
    "    # load best\n",
    "    assert best_state is not None\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    # final report\n",
    "    _, _, _, y_true, y_pred = eval_one_epoch(model, val_loader, criterion, device)\n",
    "    print(\"\\nThai2Vec feed by MLP classification report (per label):\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names, zero_division=0))\n",
    "\n",
    "    return model, label_names, device\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_multilabel(texts, preprocessing, encoder, model, label_names, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    texts: list[str]\n",
    "    return:\n",
    "      - pred_matrix: (n, n_labels) 0/1\n",
    "      - pred_labels: list[list[str]] รายชื่อ label ที่ติด\n",
    "      - proba: (n, n_labels) probability ต่อ label\n",
    "    \"\"\"\n",
    "    tovec_list, _ = preprocessing(texts)\n",
    "    X_vec = encoder.transform(tovec_list)  # (n, 300)\n",
    "\n",
    "    X_t = torch.from_numpy(X_vec).float().to(device)\n",
    "    model.eval()\n",
    "    logits = model(X_t)\n",
    "    proba = torch.sigmoid(logits).cpu().numpy()\n",
    "    pred_matrix = (proba >= threshold).astype(int)\n",
    "\n",
    "    pred_labels = []\n",
    "    for row in pred_matrix:\n",
    "        pred_labels.append([label_names[i] for i, v in enumerate(row) if v == 1])\n",
    "\n",
    "    return pred_matrix, pred_labels, proba\n",
    "\n",
    "# -------------------------\n",
    "# 5) Usage \n",
    "# -------------------------\n",
    "\n",
    "tovec_list, _ = preprocessing(X_train)\n",
    "encoder = Thai2VecEncoder()\n",
    "X_train_vec = encoder.transform(tovec_list)   # (n_docs, 300)\n",
    "\n",
    "tovec_list2, _2 = preprocessing(X_valid)\n",
    "\n",
    "X_val_vec = encoder.transform(tovec_list2)   # (n_docs, 300)\n",
    "\n",
    "label_cols = [ \"politics\",\"human_rights\",\"quality_of_life\",\"international\",\"social\",\"environment\",\"economics\",\"culture\",\"labor\",\"national_security\",\"ict\",\"education\"]\n",
    "model, label_names, device = train_mlp_from_thai2vec(X_train_vec, Y_train, X_val_vec, Y_valid, label_names=label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "372405d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: ศาลสั่งเพิกถอนคำสั่งและคุ้มครองสิทธิเสรีภาพผู้ชุมนุม\n",
      "PRED: ['international', 'environment', 'national_security', 'ict']\n",
      "\n",
      "TEXT: เศรษฐกิจชะลอจากดอกเบี้ยสูงและราคาพลังงาน\n",
      "PRED: ['international', 'environment', 'ict']\n",
      "\n",
      "TEXT: กระทรวงศึกษาฯปรับหลักสูตรใหม่และเพิ่มงบโรงเรียน\n",
      "PRED: ['human_rights', 'international', 'environment', 'ict']\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"ศาลสั่งเพิกถอนคำสั่งและคุ้มครองสิทธิเสรีภาพผู้ชุมนุม\",\n",
    "    \"เศรษฐกิจชะลอจากดอกเบี้ยสูงและราคาพลังงาน\",\n",
    "    \"กระทรวงศึกษาฯปรับหลักสูตรใหม่และเพิ่มงบโรงเรียน\"\n",
    "]\n",
    "\n",
    "pred_matrix, pred_labels, proba = predict_multilabel(\n",
    "    test_texts, preprocessing, encoder, model, label_names, device, threshold=0.5\n",
    ")\n",
    "\n",
    "for t, labs in zip(test_texts, pred_labels):\n",
    "    print(\"\\nTEXT:\", t)\n",
    "    print(\"PRED:\", labs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seen_jupyter venv1)",
   "language": "python",
   "name": "seen-jupyter-venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
